{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9RVmvnGAc8M"
      },
      "source": [
        "ECE 5460, Au25\n",
        "\n",
        "Solution for PyTorch Tutorial - final model only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VTE_nLMo2fN_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "# Task 1: Data Augmentation for Training\n",
        "# Apply transformations: flip, rotate, shift, color jitter\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
        "    transforms.RandomRotation(degrees=10),   # Random rotation up to 10 degrees\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random shift by up to 10% pixels\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color variations\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# No augmentation for validation and test sets\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "batch_size = 32  # Increased batch size for better training stability\n",
        "\n",
        "# Training set with augmentation\n",
        "ds_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "\n",
        "# Validation set without augmentation\n",
        "ds_val = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                      download=True, transform=val_test_transform)\n",
        "\n",
        "# Splitting training into train and validation\n",
        "generator = torch.Generator().manual_seed(24)\n",
        "full_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=None)\n",
        "train_indices, val_indices = torch.utils.data.random_split(\n",
        "    full_dataset, [0.8, 0.2], generator=generator)\n",
        "\n",
        "# Create custom dataset classes to apply different transforms\n",
        "class TransformDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.subset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "trainset = TransformDataset(train_indices, train_transform)\n",
        "valset = TransformDataset(val_indices, val_test_transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                        shuffle=False, num_workers=0)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=val_test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-B_apPu28ZU"
      },
      "source": [
        "## Task 1: Improved Network Architecture\n",
        "\n",
        "We will use an improved convolutional network for classification with:\n",
        "- More convolutional filters\n",
        "- Additional ReLU layer before final linear layer\n",
        "- Better feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gExfCbTp2rpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 686,282\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImprovedNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved CNN architecture with:\n",
        "    - More convolutional filters (6->32->64 instead of 6->16)\n",
        "    - Additional ReLU layer before final linear layer\n",
        "    - Better feature extraction\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # First convolutional block: 3 -> 32 filters\n",
        "        self.conv1 = nn.Conv2d(3, 32, 5, padding=2)  # padding to preserve size\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Second convolutional block: 32 -> 64 filters\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "        # Third convolutional block: 64 -> 128 filters\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        \n",
        "        # After 3 pooling operations: 32x32 -> 16x16 -> 8x8 -> 4x4\n",
        "        # So final size is 128 * 4 * 4 = 2048\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)  # 10 classes\n",
        "        self.dropout = nn.Dropout(0.5)  # Add dropout for regularization\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First conv block\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # Second conv block\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # Third conv block\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        \n",
        "        # Flatten all dimensions except batch\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        # Fully connected layers with ReLU activations\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))  # Additional ReLU layer before final layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = ImprovedNet()\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puswg-MZ3L-s"
      },
      "source": [
        "## Task 2: Tuning Learning Parameters\n",
        "\n",
        "Define a loss function and optimizer with:\n",
        "- Tuned learning rate\n",
        "- Momentum parameter\n",
        "- L2 regularization (weight decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5SBwuTVE3OrD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer: SGD with lr=0.01, momentum=0.9, weight_decay=0.0005\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Task 2: Tuned hyperparameters\n",
        "# Learning rate: reduced from 0.001 to 0.01 initially, then we'll use a scheduler\n",
        "# Momentum: 0.9 (standard value for SGD with momentum)\n",
        "# Weight decay: 5e-4 (L2 regularization)\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-4\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), \n",
        "                      lr=learning_rate, \n",
        "                      momentum=momentum, \n",
        "                      weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler to reduce LR during training\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "print(f\"Optimizer: SGD with lr={learning_rate}, momentum={momentum}, weight_decay={weight_decay}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GeOz1Gh3QQT"
      },
      "source": [
        "Define training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kteVZXEm3czt"
      },
      "outputs": [],
      "source": [
        "def train_step(model, inputs, labels):\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # forward call\n",
        "  outputs = model(inputs)\n",
        "\n",
        "  # calculate loss\n",
        "  loss = criterion(outputs, labels)\n",
        "\n",
        "  # back propagation and optimization\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_htmffPq4Cck"
      },
      "source": [
        "Define evaluation function\n",
        "\n",
        "Same can be used for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mMw9wi2G4FAb"
      },
      "outputs": [],
      "source": [
        "def val_step(model, inputs, labels):\n",
        "  # get inputs and labels from given batch\n",
        "  with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # calculate the predicted class\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    loss = criterion(outputs, labels)\n",
        "    correct_pred = (predicted == labels).sum().item() # correctly classified samples of the current batch\n",
        "  return correct_pred, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVyYxxZd4vIz"
      },
      "source": [
        "Main training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bDLwIDDe4t4V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device:  cpu\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=94, pipe_handle=108)\u001b[0m\n",
            "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mmodule '__main__' has no attribute 'TransformDataset'\u001b[0m\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m train_loss_epoch = []\n\u001b[32m     24\u001b[39m model.train()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     26\u001b[39m   inputs, labels = batch\n\u001b[32m     27\u001b[39m   inputs = inputs.to(device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/image processing/final_project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/image processing/final_project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/image processing/final_project/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1170\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1163\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1164\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/multiprocessing/context.py:288\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/multiprocessing/popen_fork.py:20\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/multiprocessing/popen_spawn_posix.py:62\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.sentinel = parent_r\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     64\u001b[39m     fds_to_close = []\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "num_epochs = 30  # Increased epochs for better training\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"using device: \", device)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "val_acc_log = []\n",
        "val_loss_log = []\n",
        "train_loss_log = []\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  running_loss = 0\n",
        "  train_loss_epoch = []\n",
        "  \n",
        "  model.train()  # Set model to training mode\n",
        "  for j, batch in enumerate(trainloader):\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # training step\n",
        "    loss = train_step(model, inputs, labels)\n",
        "    running_loss += loss.item()\n",
        "    train_loss_epoch.append(loss.item())\n",
        "\n",
        "    # print some statistics\n",
        "    if (j+1) % 500 == 0:\n",
        "      print(\"epoch {}, step = {}, running loss = {:.4f}, lr = {:.6f}\".format(\n",
        "          i, str(j+1).zfill(5), running_loss / 500, scheduler.get_last_lr()[0]))\n",
        "      running_loss = 0\n",
        "\n",
        "  # Average training loss for this epoch\n",
        "  avg_train_loss = np.mean(train_loss_epoch)\n",
        "  train_loss_log.append(avg_train_loss)\n",
        "\n",
        "  # validation at the end of each epoch\n",
        "  model.eval()  # Set model to evaluation mode\n",
        "  total_correct_pred = 0\n",
        "  val_loss = []\n",
        "  \n",
        "  for _, batch in enumerate(valloader):\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    curr_correct_pred, curr_loss = val_step(model, inputs, labels)\n",
        "\n",
        "    # keep track of corrected samples and loss\n",
        "    total_correct_pred += curr_correct_pred\n",
        "    val_loss.append(curr_loss.item())\n",
        "\n",
        "  val_loss = torch.Tensor(val_loss).mean()\n",
        "  acc = total_correct_pred / len(valset)\n",
        "  print(\"Epoch {}: val accuracy: {:.2f}%, val loss = {:.3f}, train loss = {:.3f}\".format(\n",
        "      i, 100*acc, val_loss, avg_train_loss))\n",
        "  val_acc_log.append(acc * 100)\n",
        "  val_loss_log.append(val_loss.item())\n",
        "\n",
        "  # Save best model\n",
        "  if acc > best_val_acc:\n",
        "    best_val_acc = acc\n",
        "    best_model_state = model.state_dict().copy()\n",
        "    print(f\"  -> New best validation accuracy: {100*best_val_acc:.2f}%\")\n",
        "\n",
        "  # Update learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "  # save the checkpoint\n",
        "  torch.save({\n",
        "            'epoch': i,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': acc,\n",
        "            }, 'epoch_{}_val_acc_{:.2f}.pt'.format(i, 100*acc))\n",
        "\n",
        "# Load best model\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"\\nLoaded best model with validation accuracy: {100*best_val_acc:.2f}%\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {elapsed_time/60:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(2,1)\n",
        "ax[0].plot(val_loss_log)\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].set_title('validation loss')\n",
        "\n",
        "\n",
        "ax[1].plot(val_acc_log)\n",
        "ax[1].set_xlabel('epoch')\n",
        "ax[1].set_ylabel('accuracy')\n",
        "ax[1].set_title('validation accuracy')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional Task: HoG Features + Linear Classifier\n",
        "from skimage.feature import hog\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import cv2\n",
        "\n",
        "def extract_hog_features(images):\n",
        "    \"\"\"\n",
        "    Extract HoG features from images\n",
        "    Parameters: pixels_per_cell=(8, 8), cells_per_block=(2, 2), orientations=9\n",
        "    \"\"\"\n",
        "    hog_features = []\n",
        "    for img in images:\n",
        "        # Convert from tensor to numpy and denormalize\n",
        "        img_np = img.permute(1, 2, 0).numpy()\n",
        "        img_np = (img_np + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
        "        img_np = (img_np * 255).astype(np.uint8)\n",
        "        \n",
        "        # Convert to grayscale for HoG\n",
        "        if len(img_np.shape) == 3:\n",
        "            img_gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            img_gray = img_np\n",
        "        \n",
        "        # Extract HoG features\n",
        "        features = hog(img_gray, \n",
        "                      orientations=9, \n",
        "                      pixels_per_cell=(8, 8),\n",
        "                      cells_per_block=(2, 2),\n",
        "                      feature_vector=True)\n",
        "        hog_features.append(features)\n",
        "    \n",
        "    return np.array(hog_features)\n",
        "\n",
        "# Extract HoG features from training set (sample for speed)\n",
        "print(\"Extracting HoG features from training set (this may take a while)...\")\n",
        "train_hog_features = []\n",
        "train_hog_labels = []\n",
        "\n",
        "# Sample a subset for faster computation\n",
        "sample_size = 5000\n",
        "sample_indices = np.random.choice(len(trainset), sample_size, replace=False)\n",
        "\n",
        "for idx in sample_indices[:1000]:  # Process first 1000 for demonstration\n",
        "    img, label = trainset[idx]\n",
        "    features = extract_hog_features([img])[0]\n",
        "    train_hog_features.append(features)\n",
        "    train_hog_labels.append(label)\n",
        "\n",
        "train_hog_features = np.array(train_hog_features)\n",
        "train_hog_labels = np.array(train_hog_labels)\n",
        "\n",
        "# Extract HoG features from validation set (sample)\n",
        "print(\"Extracting HoG features from validation set...\")\n",
        "val_hog_features = []\n",
        "val_hog_labels = []\n",
        "\n",
        "for idx in range(min(500, len(valset))):\n",
        "    img, label = valset[idx]\n",
        "    features = extract_hog_features([img])[0]\n",
        "    val_hog_features.append(features)\n",
        "    val_hog_labels.append(label)\n",
        "\n",
        "val_hog_features = np.array(val_hog_features)\n",
        "val_hog_labels = np.array(val_hog_labels)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "train_hog_features_scaled = scaler.fit_transform(train_hog_features)\n",
        "val_hog_features_scaled = scaler.transform(val_hog_features)\n",
        "\n",
        "# Train linear classifier\n",
        "print(\"Training linear classifier on HoG features...\")\n",
        "hog_classifier = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
        "hog_classifier.fit(train_hog_features_scaled, train_hog_labels)\n",
        "\n",
        "# Evaluate\n",
        "train_pred = hog_classifier.predict(train_hog_features_scaled)\n",
        "val_pred = hog_classifier.predict(val_hog_features_scaled)\n",
        "\n",
        "train_acc = accuracy_score(train_hog_labels, train_pred)\n",
        "val_acc = accuracy_score(val_hog_labels, val_pred)\n",
        "\n",
        "print(f\"\\nHoG + Linear Classifier Results:\")\n",
        "print(f\"Training Accuracy: {100*train_acc:.2f}%\")\n",
        "print(f\"Validation Accuracy: {100*val_acc:.2f}%\")\n",
        "print(f\"\\nNote: This is a demonstration with a subset of data. Full implementation would process all samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcV8B83ab6Tk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
        "\n",
        "# Plot losses\n",
        "ax[0].plot(train_loss_log, label='Training Loss', color='blue')\n",
        "ax[0].plot(val_loss_log, label='Validation Loss', color='red')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Training and Validation Loss')\n",
        "ax[0].legend()\n",
        "ax[0].grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "ax[1].plot(val_acc_log, label='Validation Accuracy', color='green')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_title('Validation Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].grid(True)\n",
        "ax[1].axhline(y=65, color='orange', linestyle='--', label='Target (65%)')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best validation accuracy: {max(val_acc_log):.2f}%\")\n",
        "print(f\"Final validation accuracy: {val_acc_log[-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aomKGsVO7XY2"
      },
      "source": [
        "Testing\n",
        "\n",
        "The other things to do is to calculate the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to see the accuracy of individual classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwB2EmvW7Yty"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Task 3: Calculate per-class accuracy and confusion matrix\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "total_correct_pred = 0\n",
        "\n",
        "print(\"Evaluating on test set...\")\n",
        "for i, batch in enumerate(testloader):\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        \n",
        "        curr_correct_pred = (predicted == labels).sum().item()\n",
        "        total_correct_pred += curr_correct_pred\n",
        "\n",
        "# Total accuracy\n",
        "acc = total_correct_pred / len(testloader.dataset)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Overall Test Accuracy: {100*acc:.2f}%\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Calculate per-class accuracy\n",
        "class_correct = np.diag(cm)\n",
        "class_total = cm.sum(axis=1)\n",
        "class_acc = class_correct / class_total\n",
        "\n",
        "# Print per-class accuracy\n",
        "print(\"Per-Class Test Accuracy:\")\n",
        "print(\"-\" * 60)\n",
        "for i, class_name in enumerate(classes):\n",
        "    print(f\"{class_name:10s}: {100*class_acc[i]:6.2f}% ({class_correct[i]}/{class_total[i]})\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=classes, digits=4))\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=classes, yticklabels=classes)\n",
        "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Bar plot of per-class accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors = ['green' if acc >= 65 else 'orange' if acc >= 50 else 'red' for acc in class_acc]\n",
        "bars = plt.bar(range(len(classes)), 100*class_acc, color=colors, alpha=0.7)\n",
        "plt.axhline(y=65, color='red', linestyle='--', linewidth=2, label='Target (65%)')\n",
        "plt.axhline(y=100*acc, color='blue', linestyle='--', linewidth=2, label=f'Overall ({100*acc:.2f}%)')\n",
        "plt.xlabel('Class', fontsize=12)\n",
        "plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "plt.title('Per-Class Test Accuracy', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(len(classes)), classes, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, acc_val) in enumerate(zip(bars, class_acc)):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{100*acc_val:.1f}%',\n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Summary Statistics:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Classes above 65% accuracy: {sum(class_acc >= 0.65)}/{len(classes)}\")\n",
        "print(f\"Classes above 50% accuracy: {sum(class_acc >= 0.50)}/{len(classes)}\")\n",
        "print(f\"Best performing class: {classes[np.argmax(class_acc)]} ({100*max(class_acc):.2f}%)\")\n",
        "print(f\"Worst performing class: {classes[np.argmin(class_acc)]} ({100*min(class_acc):.2f}%)\")\n",
        "print(f\"Average per-class accuracy: {100*np.mean(class_acc):.2f}%\")\n",
        "print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
